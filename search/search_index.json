{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for the TB- nanopore workshop","text":"<p>Here, you can find all of the resources for the bioinformatics sessions of the workshop.</p> <p></p>"},{"location":"bioinformatics/basecalling/","title":"Nanopore sequencing base calling and quality control","text":""},{"location":"bioinformatics/basecalling/#introduction","title":"Introduction","text":"<p>In this session we are going to be looking at data generated by third-generation nanopore sequencing technology. Developed by Oxford Nanopore Technologies (ONT), these platforms, rather than the next-generation 'sequencing-by-synthesis approach', make use of an array of microscopic protein \u2018pores\u2019 set in in an electrically resistant membrane which guide strands of DNA or RNA through them. Each nanopore corresponds to its own electrode connected to a channel and sensor chip, which measures the electric current that flows through the nanopore. When a molecule passes through a nanopore, the current is disrupted to produce a characteristic \u2018squiggle\u2019. The squiggle is then decoded using basecalling algorithms to determine the DNA or RNA sequence in real time. Oxford Nanopore\u2019s most popular platform is the MinION which is capable of generating single reads of up to 2.3 Mb (2.3 million bases).</p> <p> </p> <p>The MinION is one of 5 scalable platforms developed by ONT. High-throughput applications such as the GridION and PromethION use an array of nanopore flowcells to produce between 5 to 48 times more data than the MinION alone \u2013 outputting up to 48 TB of data in one run. More downscaled solutions such as The Flongle and SmidgION use a smaller, single flowcell to generate data. The MinION is a highly portable sequencing platform, about the size of a large USB flash drive. This technology enables researchers to perform sequencing analyses almost anywhere, providing they have the correct equipment to prepare the DNA libraries and analyse the output data. </p> <p> </p> <p>A complete sequencing run on the MinION platform can generate as much as 1 terabyte of raw data, and downstream analyses require a significant amount of compute power \u2013 multicore high performance processors and large amounts of RAM. This poses a significant logistical challenge for researchers who want to take advantage of the platform\u2019s portability aspect. Over recent years, the integration of GPUs (graphics processing units) in to analysis workflows has sped up applications involving machine learning greatly. </p> <p> </p>"},{"location":"bioinformatics/basecalling/#activity-briefing","title":"Activity Briefing","text":"<p>Today we will be working with example data, as detailed in the data section. Today, we are going to take this raw data and generate some insights to help further understand your MTB samples.  In this session today, we will:</p> <ul> <li>Basecall your data, transforming it from squiggle to bases.</li> <li>Perform quality control on the data, ensuring our data is of sufficient quality for analysis.</li> <li>Map the reads to a reference genome, producing three genomic </li> </ul>"},{"location":"bioinformatics/basecalling/#basecalling","title":"Basecalling","text":"<p>To convert the raw data output produced by the MinION sequencing run in to a usable form we need to perform a process called basecalling. This converts the raw electronic signal which is collected as the DNA passes through the pore, in to base reads \u2013 A, C, T or G. To do this we will use a program called Guppy \u2013 a software package designed by ONT which uses recurrent neural nets (RNN) to interpret the raw signal, which comes in a proprietary '.fast5' format file produced by the sequencer software and convert it in to the standard .fastq format, for use downstream in our pipeline. Users also have the choice of using the experimental Dorado basecaller which gives the option of training specialised models for specific basecalling applications. As mentioned above, GPUs are used to accelerate the basecalling process. Without a GPU performing basecalling becomes a very slow process, therfore it is advised that users procure a machine with a compatable Nvidia GPU (more information on this here).</p> <p> </p> <p>Activate the relevant conda environment, navigate to the <code>~/data/example_data/</code> basecalling folder in the home directory, and we\u2019ll start the first step.</p> <p>Important</p> <p>Every time you open a new terminal window, you must re-activate the nanopore conda environment. conda activate nanopore</p> <pre><code>conda activate basecalling\n</code></pre> <pre><code>cd ~/data/example_data/basecalling/fast5 \n</code></pre> <p>Use the <code>ls</code> command to see what is inside this folder. Use <code>head</code> to preview one of the fast5 files. As you might find, it's completely unreadable. This is because at this stage, the data is in a binary format representing the squiggle signal we spoke about previously. We need to basecall this data before we can use it.</p> <p>Basecalling can be performed in a number of ways. There is an option to perform this while sequencing in the MinKNOW GUI, however this software provides fewer options in the ways basecalling can be completed. Here, we will use Guppy for maximum flexibility. Since the machines we are working on do not have a GPU available we will have to use the two CPU cores available to us. Therefore, we will only basecall a subset (&lt;1%) of the dataset as an example, and in the subsequent steps we will use a pre-basecalled output. Copy the whole line in to the terminal and execute the command:</p> <pre><code>guppy_basecaller --config ~/bin/ont-guppy-cpu/data/dna_r9.4.1_450bps_fast.cfg --trim_adapters --detect_barcodes --compress_fastq  --input_path ~/data/example_data/basecalling/fast5  --save_path ~/data/example_data/basecalling/fastq\n</code></pre> <p>We used the the dna_r9.4.1_450bps_fast model for basecalling as this data was generated with a r9.4.1 chemistry, we also using a model which runs fast but produces lower accuracy data. The model should be adjusted based on the chemistry (e.g. r10.4.1) and the desired speed (fast, higher and superior accuracy). A number of models are listed below.</p> Model Accuracy Chemistry Flow cell dna_r9.4.1_450bps_fast.cfg Low Kit 9 R9.4.1 dna_r9.4.1_450bps_hac.cfg Med Kit 9 R9.4.1 dna_r9.4.1_450bps_sup.cfg High Kit 9 R9.4.1 dna_r10.4.1_e8.2_400bps_fast.cfg Low Kit 14 R10.4.1 dna_r10.4.1_e8.2_400bps_hac.cfg Med Kit 14 R10.4.1 dna_r10.4.1_e8.2_400bps_sup.cfg HIgh Kit 14 R10.4.1 <p>You should now see the bascalling process begin, and a progress bar appear. This may take some time depending on the performance of your machine.</p> <p>When the process in completed, you will find the basecalled reads in a .fastq formatted file. Navigate there by typing the following in to the terminal:  </p> <pre><code>cd ~/data/example_data/basecalling/fastq/pass\n</code></pre> <p>Use the <code>ls</code> command to see what is inside this folder. This directory holds several folders - one for each barcode found by guppy. Each of these folders contains the fastq formatted 'pass' reads from the basecalling process. The reads have a quality score &gt; 7. Use  <code>zcat FILENAME | head</code> (remebering to use the correct filename) to preview the compresssed fastq file. Unlike the fast5 files, these are human-readable and contain all of the read data required for downstream analyses. Can you identify any of the common elements of a .fastq format files - similar to the ones you may have encountered in previous sessions? Click here to find out more about the FASTQ format.</p>"},{"location":"bioinformatics/basecalling/#basecalling-quality-control","title":"Basecalling - Quality Control","text":"<p>Before moving on to the analysis steps, it is important to gauge the quality of your sequencing output. There are numerous factors which dictate the quality of the output data, spanning between quality of the input material, library preparation to software and hardware failure. We will look at some important metrics produced by the sequencer which will give us a feel for how well the run went. In order to get the run metrics in to a useful form, we will use an pycoQC to produce a range of plots in a HTML output, which we will use to judge the quality of the sequencing run. Something to note, is that in this activity we will only use a small subset of the sequenced reads, or else the analysis would take all day. This subsetting means that the sequencing telemetry may look inconsistent, when compared to a full run.</p> <pre><code>pycoQC -f ~/data/example_data/basecalling/fastq/.sequencing_summary.txt -o ~/data/example_data/basecalling/fastq/pycoqc_results.html\n</code></pre> <p>After executing the command you should find a file called 'pycoqc_results.html'. Open them up in the file manager or in the terminal (with the below command) and inspect some of the plots and see what you can find out. As mentioned, the data here are only a small subset of reads, so some of the plots are incomplete. But this should give you a good idea of how this analysis should look.</p> <pre><code>firefox ~/data/example_data/basecalling/fastq/pycoqc_results.html\n</code></pre> <p>Before continuing, quit firefox by clicking the X in the top right corner of the web-browser window.</p> <p>Exercise 1</p> Question 1Answer 1 <p>Approximately how long did the sequencing run take?</p> <p>You can find the answer in the 'General run summary' table.</p> <p>48.06 hours</p> <p>Exercise 2</p> Question 2Answer 2 <p>What is the N50 of the passed reads (&gt;Q7) for all basecalled reads in this run?</p> <p>Try looking at the table at the top of the pycoQC report. Be sure to find the 'Passed' read section.</p> <p>1990</p>"},{"location":"bioinformatics/basecalling/#adapter-trimming","title":"Adapter Trimming","text":"<p>Nanopore library preparation results in the addition of a sequencing adapter at each end of the fragment. Both the template and complement strands to be sequenced carry the motor protein which means both strands are able to translocate the nanopore. For downstream analysis, it is important to remove these adapters. For this we will use Porechop. This program processes all of the reads in our basecalled fastq file, and removes these adapter sequences. Furthermore, the ligation library prep process can result in conjoined reads, meaning an adapter will be found in the middle of an extra-long read. Porechop will identify these, split them and remove the adapters. In addition, if you use a multiplexing kit to maximise sample throughput, this program will split the reads based on the molecular barcode added to each sample. Our dataset only has one sample, so this demultiplexing won't be necessary. </p> <p>Let's launch porechop and remove the adapters from the basecalled fastq file of barcode01.</p> <pre><code>cd ~/data/example_data/basecalling/fastq\n\nporechop -i ~/data/example_data/basecalling/fastq/pass/barcode01 -o barcode01.porechop.fastq\n</code></pre> <p>Read the output of the terminal to understand better what porechop is doing to the dataset. Ask a demonstrator if you have any questions about this.</p>"},{"location":"bioinformatics/basecalling/#kraken-qc","title":"Kraken QC","text":"<p>Another method of quality control is to check our reads for sequence contamination from other 'off-target' organisms. This is important in order to firstly, understand how effective your DNA extraction, enrichment and sequencing was. And secondly, to prevent anomalous reads from being incorporated in to assemblies. Using our basecalled reads we will perform an analysis using Kraken. Kraken is a tool which sifts through each read in a .fastq file and crosschecks it against a database of microorganism genomes. The output is a taxonomic assignment of each read, enabling to identify if any contamination has occurred. In this case we will be looking for any reads which do not belong to the Plasmodium knowlesi genome. Let\u2019s navigate to the kraken folder to begin the analysis:</p> <pre><code>cd ~/data/example_data/kraken\n</code></pre> <p>The following line of code is composed of these elements:</p> <ul> <li><code>kraken2</code> \u2013 calling the kraken2 executable</li> <li><code>--db ~/tb-kraken</code> - this points kraken to a vast sequence database of relevant microorganisms to cross-check our reads against</li> <li><code>--output sample1.koutput.txt</code> \u2013 this argument locates the output file</li> <li><code>~/data/example_data/sample1.fastq.gz</code> \u2013 this argument locates the input file</li> </ul> <p>As before, to save time, we will run Kraken on only one sample. Type the following command in to the terminal to unleash the Kraken:</p> <pre><code>kraken2 --db ~/tb-kraken --output sample1.koutput.txt ~/data/example_data/sample1.fastq.gz\n</code></pre> <p>This file isn't particularly easy to interpret, so we will use a program called Recentrifuge to transform these data in to a more human-readable format.</p> <pre><code>rcf -k sample1.koutput.txt -o sample1.rcf.html --nodespath ./taxdump\n</code></pre> <p>Try opening the HTML file generated by recentrifuge in a web browser, what can you tell about the sequencing run? Was is successful? Note - due to constraints with the virtual machine, we have generated an alternative report, which can be loaded using the below command. Copy and paste it in to the terminal. If you have any questions about this, ask a demonstrator.</p> <pre><code>firefox sample1.rcf.html\n</code></pre> <p>Exercise 3</p> Question 3Answer 3 <p>Run this analysis for sample 2. Can you see any contaminating reads? What organism(s) is/are there?</p> <p>Looks like we have a mixture of M. intracellulare and M. tuberculosis. Why do you think this might be?</p> <p>Cheat sheet</p> <pre><code>conda activate basecalling\ncd ~/data/example_data/basecalling/fast5 \nguppy_basecaller --config ~/bin/ont-guppy-cpu/data/dna_r9.4.1_450bps_fast.cfg --trim_adapters --detect_barcodes --compress_fastq  --input_path ~/data/example_data/basecalling/fast5  --save_path ~/data/example_data/basecalling/fastq\ncd ~/data/example_data/basecalling/fastq/pass\npycoQC -f ~/data/example_data/basecalling/fastq/.sequencing_summary.txt -o ~/data/example_data/basecalling/fastq/pycoqc_results.html\nfirefox ~/data/example_data/basecalling/fastq/pycoqc_results.html &amp;\ncd ~/data/example_data/basecalling/fastq\nporechop -i ~/data/example_data/basecalling/fastq/pass/barcode01 -o barcode01.porechop.fastq\ncd ~/data/example_data/kraken\nkraken2 --db ~/tb-kraken --output sample1.koutput.txt ~/data/example_data/sample1.fastq.gz\nrcf -k sample1.koutput.txt -o sample1.rcf.html --nodespath ./taxdump\nfirefox sample1.rcf.html &amp;\nkraken2 --db ~/tb-kraken --output sample2.koutput.txt ~/data/example_data/sample2.fastq.gz\nrcf -k sample2.koutput.txt -o sample2.rcf.html --nodespath ./taxdump\nfirefox sample2.rcf.html &amp;\n</code></pre>"},{"location":"bioinformatics/data/","title":"The data","text":""},{"location":"bioinformatics/data/#background","title":"Background","text":"<p>We have provided you with sequencing data from 4 samples that were collected in Portugal from individuals with MDR-TB and XDR-TB and were sequenced using the minION as part of an evaluating of the technology to detect resistance1. The samples were also tested using for drug resistance using phenotypic methods2. </p> <p>Drug susceptibility testing</p> <p>All clinical samples were prepared by inoculating a single colony into Middlebrook 7H9 broth supplemented with 10% OADC (Becton Dickinson). Susceptibility testing for the first-line anti-TB drugs rifampicin (RIF), isoniazid (INH), ethambutol (ETB), pyrazinamide (PZA) and streptomycin (STR) and the second-line drugs rifabutin (RFB), amikacin (AMK), capreomycin (CAP), ofloxacin (OFX), moxifloxacin (MOX), ethionamide (ETH), para-aminosalicylic acid (PAS) and linezolid (LZ) was performed on all strains with the MGIT960 system (Becton Dickinson), according to the manufacturer\u2019s instructions. Quantitative drug susceptibility testing (qDST) for both first- and second-line drugs was conducted using a combination of the MGIT960 system and the Epicenter V5.80A software equipped with the TB eXIST module (Becton Dickinson)</p>"},{"location":"bioinformatics/data/#phenotype","title":"Phenotype","text":"ID sample1 sample2 sample3 sample4 Isoniazid R R R R Rifampicin R R R R Streptomycin R S R R Ethambutol R S R R Pyrazinamide R S R R Amikacin R S R S Kanamycin R - R S Capreomycin R S R S Ofloxacin R S R S Moxifloxacin R S R S Ethionamide R R R R Linezolid S S S S Resistance phenotype XDR-TB MDR-TB XDR-TB MDR-TB <ol> <li> <p>Phelan, J. E. et al. Integrating informatics tools and portable sequencing technology for rapid detection of resistance to anti-tuberculous drugs. Genome Med 11, 41 (2019).\u00a0\u21a9</p> </li> <li> <p>Phelan, J. et al. The variability and reproducibility of whole genome sequencing technology for detecting resistance to anti-tuberculous drugs. Genome Medicine 2016 8:1 8, 1\u20139 (2016).\u00a0\u21a9</p> </li> </ol>"},{"location":"bioinformatics/intro-to-linux/","title":"Linux","text":"<p>Most bioinformatic software is written for Unix-type systems and is usually operated from the command line. This means that these programs are intended to be used by typing commands and they don't have a graphical user interface (GUI). There are many reasons for this, but two important ones are that command line programs are quicker to develop and also very easy to automate. To get from raw sequences to final results often requires many steps. Imagine processing thousands of isolates: this would potentially involve hundreds of hours and even more mouse clicks if performed with a GUI. Using a command line tool, a workflow can be defined by chaining a series of commands together in a script which can then be rapidly applied to thousands of isolates without much effort. </p> <p>Over the duration of this course you will be learning the skills required to independently undertake bioinformatic projects. By the end of this course you should be comfortable to go all the way from raw data to biological insights.</p> <p>In order to learn how to use bioinformatic programs we must first become comfortable with how to use the terminal. This may be daunting if you have never used a command line environment before, but hopefully by the end of this practical session you should have a basic understanding which will be reinforced over the next few sessions.</p> <p>Let's start by opening a terminal by clicking on the icon on the sidebar. Your screen should now look like the one below:</p> <p></p> <p>The terminal is very similar to the file explorer in other operating systems. It allows you to see files and interact with them. The only difference is that we have to do this by typing a command into the terminal window. To demonstrate this, let's open up Linux's file explorer by clicking on the icon in the side-bar on the left-hand side of the screen. When you open the file explorer it will go to your home folder and show you all the sub-folders and files present. The home folder is simply the root folder which contains all your files and folders.</p> <p>You can see a number of sub-folders and files present in the file explorer. Let's see if we can find them in the terminal. To do this, we have to type ls into the terminal and hit enter. You should now see a screen similar to the one below. Now you can see the same files and folders in the terminal. </p> <p></p> <p>In the file explorer double click on the data folder. You will see several new folders which hold the data for the proceeding practicals. Let's try find them using the terminal!</p> <p>When we double clicked on the data folder we effectively moved from the home folder to the data folder. We will have to do the same using the terminal. To do this, type in <code>cd data</code> and hit enter. Now try using ls again to produce the same list of folders that you see in the file explorer.</p> <p>Info</p> <p>Folders are often referred to as directories in Linux. We will use directory from now on, but the two words are synonymous. </p> <p>You will notice that text in front of the $ sign changed when you moved from home to data. You should see something similar to what is shown below: </p> <p>Terminal output</p> <pre><code>user@user-VirtualBox:~/data$ \n</code></pre> <p>The blue text shows the location where the terminal is currently. In this case we are in <code>~/data</code>. Another name for this is the current directory. The <code>~</code> character is a special character which symbolises your home directory. So, we can interpret this as \"We are currently in the data directory which is in the home directory\"</p> <p>Question</p> <p>Another way of finding the current location of the terminal is by using pwd which stands for print working directory. Try using this command. Does it produce a similar output to what we have seen above? </p> <p>We have now successfully used the terminal and the <code>ls</code>, <code>cd</code> and <code>pwd</code> commands. It is as simple as that! Some commands can be used by themselves such as <code>ls</code>, however others such as <code>cd</code> need additional information called arguments to do something useful. In the example above, we need to tell <code>cd</code> that we wanted to go to the <code>data</code> directory, so we typed <code>data</code> after <code>cd</code> separated by a space. While <code>cd</code> only took one argument, some programs take many more. You will see examples of this in a bit.</p> <p>Now that you have mastered using the terminal you can close the file explorer, this is the last time we will use it. We must face the long dark of the terminal! </p>"},{"location":"bioinformatics/intro-to-linux/#useful-commandsprograms","title":"Useful commands/programs","text":""},{"location":"bioinformatics/intro-to-linux/#head","title":"head","text":"<p>Change to the tb directory and have a look at the files. Hopefully you will be able to see the tb.fasta file. The file is just a very large text file which stores the sequence data of the M. tuberculosis reference genome. We can use <code>head</code> to take a look at the first few lines of a file. Let's try it: </p> <pre><code>cd ~/refgenome\nls\nhead tb.fasta\n</code></pre> <p>Hopefully you will see something like this: </p> <p>Terminal output</p> <pre><code>&gt;Chromosome\nTTGACCGATGACCCCGGTTCAGGCTTCACCACAGTGTGGAACGCGGTCGTCTCCGAACTTAACGGCGACC\nCTAAGGTTGACGACGGACCCAGCAGTGATGCTAATCTCAGCGCTCCGCTGACCCCTCAGCAAAGGGCTTG\nGCTCAATCTCGTCCAGCCATTGACCATCGTCGAGGGGTTTGCTCTGTTATCCGTGCCGAGCAGCTTTGTC\nCAAAACGAAATCGAGCGCCATCTGCGGGCCCCGATTACCGACGCTCTCAGCCGCCGACTCGGACATCAGA\nTCCAACTCGGGGTCCGCATCGCTCCGCCGGCGACCGACGAAGCCGACGACACTACCGTGCCGCCTTCCGA\nAAATCCTGCTACCACATCGCCAGACACCACAACCGACAACGACGAGATTGATGACAGCGCTGCGGCACGG\nGGCGATAACCAGCACAGTTGGCCAAGTTACTTCACCGAGCGCCCGCACAATACCGATTCCGCTACCGCTG\nGCGTAACCAGCCTTAACCGTCGCTACACCTTTGATACGTTCGTTATCGGCGCCTCCAACCGGTTCGCGCA\nCGCCGCCGCCTTGGCGATCGCAGAAGCACCCGCCCGCGCTTACAACCCCCTGTTCATCTGGGGCGAGTCC\n</code></pre> <p>We can see that by default <code>head</code> prints out the first 10 lines. We can modify this number by providing an additional parameter. </p> <pre><code>head -5 tb.fasta\n</code></pre> <p>This command it will print out the first five lines instead. Here we have used an optional parameter (the program will still function properly without it). Optional parameters are usually given by providing two values, in this case <code>-n</code> and <code>5</code>. The first value specified what parameter we want to provide (e.g. <code>-n</code> = number of lines) and the second value is that which the parameter must take (e.g. <code>5</code> will tell the command to print the first five lines). </p>"},{"location":"bioinformatics/intro-to-linux/#less","title":"less","text":"<p>Another tool that we can use to view text files is the <code>less</code> command. Let's try view the reference file again with this method. </p> <pre><code>less tb.fasta\n</code></pre> <p>This will open an interactive viewer which fills up the screen. You can move up and down the file using the up and down keys on the keyboard. When you are finished viewing, hit the <code>q</code> key to quit the viewer. </p>"},{"location":"bioinformatics/intro-to-linux/#cp","title":"cp","text":"<p>If we need to copy a file we can use the <code>cp</code> command. We need to give two arguments: the file we want to copy and the filename of the copy. Let's try it: </p> <pre><code>cp tb.fasta tb_copy\n</code></pre> <p>Info</p> <p>Two things to note here:</p> <ol> <li>Spaces are not allowed in file names! Always use an underscore instead of a space.</li> <li>We have not given our new file an extension. Extensions are useful as they tell us something about how a file is formatted and what it might contain. If we come back a year later, we might have forgotten that <code>tb_copy</code> contained sequence data.</li> </ol>"},{"location":"bioinformatics/intro-to-linux/#mv","title":"mv","text":"<p>Let's give our file an extension. To do this we have to rename the file. The command for this is <code>mv</code> which stands for move. This command allows you to both move and rename files. Try to add an extension to the file by using the following code: </p> <pre><code>mv tb_copy tb_copy.fasta\n</code></pre> <p>You can see that this command also required two parameters:</p> <ol> <li>The file we want to rename</li> <li>The new file name</li> </ol>"},{"location":"bioinformatics/intro-to-linux/#rm","title":"rm","text":"<p>We now have two files with the same content. Let's remove our copy of the reference by using the <code>rm</code> command: </p> <pre><code>rm tb_copy.fasta\n</code></pre> <p>Danger</p> <p>Be very careful with this command! Once you remove a file using rm there is no way of getting it back. Always double check you are removing the right file before hitting enter. </p>"},{"location":"bioinformatics/intro-to-linux/#pipes","title":"Pipes","text":"<p>Imagine a cake factory production line. There are many steps that need to be taken to build the final cake (different ingredients added). Let's say we have 5 people working to make 100 cakes per day and each person specialises in adding a particular ingredient. We could get 100 bowls and add the flour to each one, then add eggs to each one and repeat this with each ingredient until we have the complete mix however we would be losing a lot of time as at any point there would be only one person working while the others wait for their turn. A more efficient way to do this by installing in a conveyor belt and having the employees sequentially add their ingredients to each bowl. This way everyone is working at the same time and the cakes will be made a lot faster. </p> <p></p> <p>The same principals apply to bioinformatic analyses. For example, take a look at the steps required to print the 9th and 10th line of the tb reference fasta file. The easiest way to do this is to:</p> <ol> <li>first extract the first 10 lines</li> <li>take the last two lines of that extract</li> </ol> <p>There are two jobs to be done and the first job passes its data to the second job. This is a good place to use pipes. We can use <code>head</code> to get the first 10 lines as we have done above. We will also intoduce <code>tail</code> which can be used to print the last lines. Try running this command:</p> <pre><code>head -n 10 tb.fasta | tail -n 2\n</code></pre>"},{"location":"bioinformatics/intro-to-linux/#troubleshooting","title":"Troubleshooting","text":"<p>Sometimes an error may occur while trying to run a command. There may be a number of different reasons for this and troubleshooting is an important skill to master. </p> <p>There are a number of ways to check if your commands have failed including:</p> <ol> <li>Run time: Some commands (such as alignment) are expected to run for at least a few minutes. If the command finished instantly and you do not expect it to do so then something has probably gone wrong.</li> <li>Error messages: Usually if a command has failed it will print out an error message. Check the last few lines of output from a program and check to see if you see any.</li> </ol> <p>There are some errors which are are commonly made. We have listed a few below. </p>"},{"location":"bioinformatics/intro-to-linux/#file-or-folder-not-present","title":"File or folder not present","text":"<p>These types of error may present in different ways depending on the command used. For example, let's try change to a directory that doesn't exist. </p> <pre><code>cd fake_directory\n</code></pre> <p>You should see something like this: </p> <p>Terminal output</p> <pre><code>cd: fake_directory: No such file or directory \n</code></pre> <p>This tells us that <code>cd</code> could not find the directory called <code>fake_directory</code>. </p> <p>Info</p> <p>Remember that the terminal is case sensitive and will also not tolerate any spaces in places where they shouldn't be. For example try running this command and see what you get:</p> <pre><code>head TB.fasta\n</code></pre> <p>Can you fix it?</p>"},{"location":"bioinformatics/intro-to-linux/#missing-a-required-argument","title":"Missing a required argument","text":"<p>If you run a program and fail to specify a it may give an error. Alternatively, it may display the program usage help and quit. For example, lets run <code>head</code> without passing the number of lines to the `-n`` flag: </p> <pre><code>head -n tb.fasta\n</code></pre> <p>You should see an error similar to that displayed below: </p> <p>Terminal output</p> <pre><code>head: invalid number of lines 'tb.fasta' \n</code></pre> <p>The error is a bit cryptic but from the illegal line count, we can see that whatever parameter we passed to <code>-n</code> (which determines how many lines are printed) is not a valid value. It turns out that this value always has to be a number, however since we left out that parameter it parses tb.fasta as the value and that causes the error. </p>"},{"location":"bioinformatics/intro-to-linux/#command-not-found","title":"Command not found","text":"<p>The different software used for the practicals are installed in \"conda environments\". These allow to have multiple versions of the same program to be installed. If you run into a <code>command not found</code> error, it is always a good idea to check if the correct conda environment for the current practical is activated (the name of the currently active environment is usually printed in parenthesis to the left of your prompt). You can activate an environment with </p> <pre><code>conda activate environment_name\n</code></pre> <p>Make sure to replace \"environment_name\" with your chosen environment. You can check which environments are available with:</p> <pre><code>conda env list\n</code></pre> <p>Question</p> <p>Try activate the tb-profiler environment. Did it work?</p>"},{"location":"bioinformatics/mapping/","title":"Aligning reads","text":""},{"location":"bioinformatics/mapping/#intended-learning-outcomes","title":"Intended learning outcomes","text":"<p>This practical session goes into how to align long-read sequencing data from ONT to a reference genome:</p> <ol> <li>Define the process of mapping sequence data</li> <li>Describe the different softwares used in the process</li> <li>Apply the commands to new data</li> </ol>"},{"location":"bioinformatics/mapping/#background","title":"Background","text":"<p>Aligning short reads from next-generation sequencing platforms such as Oxford Nanopore Technology (ONT) to a reference genome is an essential step in the data analysis pipeline for several compelling reasons. Firstly, ONT generates millions of reads, typically around a couple of kb in length, making them challenging to interpret individually. By aligning these reads to a reference genome, we can accurately map each read's origin and reconstruct the underlying genomic sequence. This process enables the detection of genetic variations, such as single-nucleotide polymorphisms (SNPs) and structural variants, aiding in the understanding of genetic diversity and finding important mutations such as those that casue resistance.</p> <p>Info</p> <p>Usually we refer to the alignment of reads to a reference as mapping. You might see these used interchangebly in the literature and in protocols.</p> <p>To perform mapping of the reads we need to use a reference genome. Mycobacterium tuberculosis (Mtb) does not have a large amount of genomic diversity compared to other bacteria and for most purposes we can use the sequence of the H37Rv lab strain which was published in 1998 by Cole et al1. We have downloaded the reference sequence for you and have put it in the <code>~/refgenome/</code> directory under the name <code>tb.fasta</code>.</p> <p>The FASTA format is a widely used text-based file format for representing biological sequence data. It is named after the program FASTA, which introduced this format for sequence similarity searching. In a FASTA file, each biological sequence (DNA, RNA, or protein) is represented by two parts: a single-line sequence identifier, beginning with the \"&gt;\" symbol, followed by a multi-line sequence data section. The sequence data contains the actual string of letters representing the sequence itself. FASTA format is simple, human-readable, and allows for easy exchange of biological sequence data among researchers and bioinformatics tools. Its versatility and popularity make it an essential standard for storing and sharing genetic and protein sequence information in various bioinformatics applications and databases.</p> <p>Exercise</p> QuestionAnswer <p>Try to use the skills you learned in the into to linux session to take a look at the fasta file and check it matches the description above.</p> <p>You can use the <code>head</code> command to look at the file <pre><code>head ~/refgenome/tb.fasta\n</code></pre></p> <p>Let's find out some more about the TB sequence. We can use a tool called <code>seqkit</code> for this, this will give you basic information such as the length of sequence and the number of sequences.</p> <pre><code>seqkit stats ~/refgenome/tb.fasta\n</code></pre> <p>Exercise</p> QuestionAnswer <p>What is the length of the genome sequence?</p> <p>You should be able to see that the \"sum_len\" column reports 4,411,532 which is the number of basepairs in the genome of Mtb.</p> <p>The sum_len, min_len, avg_len and max_len columns all report the same value because there is only one sequence in the tb genome. These columns are more useful to look at when analysing files with multiple sequences, for example, the fastq files we get from the minION.</p> <p>The basecalled data is in the <code>~/data/example_data</code> folder. Change the working directory to where the data is using the the following command:</p> <pre><code>cd ~/data/example_data\n</code></pre> <p>Exercise</p> QuestionAnswer <p>What is the mean length of the reads for the different samples? Which sample has the most reads?</p> <p>You can use seqkit to find this by running:</p> <pre><code>seqkit stats *.fastq.gz\n</code></pre> <p>This will give you the stats for all four files. Sample1 has the most reads with 278,337 reads.</p> <p>Info</p> <p>The asterisk (\"*\") represents zero or more characters. When used in commands, it matches any sequence of characters. In the example above it matchs all files with a \".fastq.gz\" extension, regardless of the filename's prefix or length. You can try this out by running the following command:</p> <pre><code>echo *.fastq.gz\n</code></pre>"},{"location":"bioinformatics/mapping/#software","title":"Software","text":"<p>First things first let's activate the right environment</p> <pre><code>conda activate mapping\n</code></pre>"},{"location":"bioinformatics/mapping/#minimap2","title":"Minimap2","text":"<p>We are now going to align our reads to the H37Rv reference using a tool called <code>minimap2</code>, which is a versatile and efficient sequence alignment tool primarily used for mapping DNA or RNA sequences against reference genomes. It is designed to handle long-read sequencing data, which is generated by technologies like PacBio or Oxford Nanopore. To perform its alignment tasks, Minimap2 requires the following inputs:</p> <ol> <li> <p>Query Sequences: These are the sequences that you want to align to the reference genome. Query sequences can be in FASTA or FASTQ format and typically represent DNA or RNA sequences generated from sequencing technologies like PacBio or Oxford Nanopore.</p> </li> <li> <p>Reference Genome: Minimap2 needs a reference genome against which the query sequences will be aligned. The reference genome is typically a well-assembled, known genome, represented in a FASTA format. It acts as a blueprint to which the query sequences are compared and aligned. It also takes in some optional parameters such as the type of input data (e.g. ONT or PacBio). </p> </li> </ol>"},{"location":"bioinformatics/mapping/#samtools","title":"Samtools","text":"<p>We will also utilise <code>samtools</code> to store the aligned data in an appropriate format. Samtools is a widely used and powerful software suite for processing and manipulating data from high-throughput DNA sequencing experiments. Its name is derived from the \"Sequence Alignment/Map\" format, commonly known as SAM. Samtools allows researchers to work with sequence data in SAM, BAM (binary version of SAM), and CRAM (compressed version of SAM) formats. Key functionalities of <code>samtools</code> include file format conversion, sorting and indexing. </p> <p>Samtools is an essential component in many bioinformatics pipelines and is widely adopted by researchers and bioinformaticians to process, analyze, and interpret next-generation sequencing data. Its efficiency, versatility, and open-source nature have contributed to its popularity in the genomics community. </p>"},{"location":"bioinformatics/mapping/#running-mapping","title":"Running mapping","text":"<p>To run the mapping analysis for sample1 please run the following command:</p> <pre><code>minimap2 -a -x map-ont ~/refgenome/tb.fasta sample1.fastq.gz | samtools sort - -o sample1.bam\n</code></pre> <p>There are a few things going with this command so lets go through each component. First of all you'll probably notice that there is a pipe (|) present so this means that there are two jobs running in series. The first one if <code>minimap2</code> which passes its processed data to <code>samtools</code>. We'll go through the <code>minimap2</code> part first. </p> <p>The <code>-a</code> argument tells minimap to output in SAM format. The <code>-x map-ont</code> argument tells the software that we are mapping ONT data and as such the process will be optimised for this. </p> <p>The SAM format data is then passed to samtools using a pipe. We use the <code>samtools sort</code> function to sort the data and store it in a binary compressed version of SAM called BAM format. Samtools can either read from an existing file or read from a pipe and this is given using the first command. We used the <code>-</code> character to tell samtools that we are reading from a pipe. The <code>-o sample1.bam</code> argument tells samtools to store the output in a file called sample1.bam. </p>"},{"location":"bioinformatics/mapping/#indexing","title":"Indexing","text":"<p>BAM files are binary representations of aligned sequencing data, and indexing these files enhances data accessibility and dramatically speeds up specific operations. Indexing allows software to quickly retrieve data from specific genomic regions, making it essential for tasks such as variant calling, visualization, and downstream analyses. Without an index, tools would need to scan the entire BAM file sequentially, resulting in significantly slower processing times, especially for large datasets. Additionally, indexed BAM files are required for many bioinformatics tools to function correctly, ensuring proper data manipulation, extraction, and filtering. We can index the bam file using samtools:</p> <pre><code>samtools index sample1.bam\n</code></pre>"},{"location":"bioinformatics/mapping/#quality-control","title":"Quality control","text":"<p>As usual we need to check the quality of the alignments. There are a few quick metrics that we can check to see if the process has worked correctly. These include:</p> <ol> <li>The total number of reads mapped</li> <li>The percentage of total reads that mapped to the reference</li> <li>The average depth of coverage across the genome</li> <li>The percentage of the genome that is covered by our minimum desired depth</li> </ol> <p>For all these metrics, higher values are better and indicate better coverage across the genome. We can use <code>samtools</code> to do the calculations. The first two can be computed using the following command:</p> <pre><code>samtools flagstat sample1.bam\n</code></pre> <p>This will print out a few different numbers two of which include the total number of reads mapped and the percentage of reads that mapped. </p> <p>Metrics 3 and 4 can be calculated by first using bedtools to calculate the depth of coverage across every site in the genome. We can do this with the following command:</p> <pre><code>bedtools genomecov -ibam sample1.bam &gt; sample1.cov.txt\n</code></pre> <p>Have a look at the file by using <code>less</code> or <code>head</code>. The columns show:</p> <ol> <li>Chromosome name</li> <li>Depth</li> <li>Number of bases at that depth</li> <li>Total length of the genome</li> <li>Fraction of the genome at that depth</li> </ol> <p>A good coverage for downstream analysis would be &gt;=20x. We can work this out manually usuing the coverage file or we can do it automatically with a script that we have written for you:</p> <pre><code>genomecov_summary.py -i sample1.cov.txt -c 20\n</code></pre> <p>This will print the percentage of the genome that is covered by 20x. You can change the <code>-c</code> number if you need to be more or less stringent.</p> <p>Exercise</p> QuestionAnswer <p>Try run this for the other sample bams. Do they all have sufficient coverage?</p> <p>Sample 2 has very low coverage and is not sufficient for downstream analysis.</p>"},{"location":"bioinformatics/mapping/#viewing-the-alignments","title":"Viewing the alignments","text":"<p>Another way to look at the quality of your alignments is by using a genome browser. The Integrative Genomics Viewer (IGV) is a powerful and widely used genome browser that facilitates the visualization and analysis of genomic data. Developed by the Broad Institute, IGV provides researchers with a user-friendly interface to explore various genomic data types, including next-generation sequencing data. Users can view aligned sequence reads, gene annotations, and genomic features, as well as compare multiple samples and datasets simultaneously. Open up the GUI by entering <code>igv</code> in to the terminal. You should see a window pop up pre-loaded with the Mtb genome. </p>"},{"location":"bioinformatics/mapping/#loading-your-bam-file","title":"Loading your bam file","text":"<p>Load your bam files by clicking on \"File\" -&gt; \"Load from File...\" and then selecting the bam files for sample 1 and sample 2 that you created earlier.</p> <p>Important</p> <p>IGV requires an index file for all the bam files you are loading. If you get an error please check if you have run this step.</p> <p>There is a lot going on on the screen so have a look at the image below which explains some of the most important features.</p> <p></p> <ol> <li>Zoom controls: This controls the level of zoom. To see your alignments you will need to zoom in</li> <li>Search bar: Here you can specify the region you want to view using genomic coordinates in the form of Chromosome:start-end. You can also search by gene name.</li> <li>Coverage track: This displays when you have loaded a bam file and is a bargraph where each bar represents a genomic position and the high is determined by the number of reads aligning to that position. If you click on a bar it will open up a window telling you the number of ACTGs. </li> <li>Alignment track: Each grey bar represents a read. If there are coloured nucleotides, black dashes or purple vertical lines it represents a difference from the reference. Have a look at this document to find out more.</li> <li>Gene track: This shows the location and orientation of the genes. Click on a gene to find out more information.</li> </ol> <p>Exercise</p> <p>Have a go at exploring your alignments. What differences do you see between the two samples you have loaded?</p>"},{"location":"bioinformatics/mapping/#identifying-mutations","title":"Identifying mutations","text":"<p>You might have noticed a lot of colour on both the the coverage track and the alignment track. This indicates that the aligned data differs from the reference and this could arise from a mutation. It is important to distinguish between a true mutation and an error from the sequencer. A high depth of sequencing is essential in next-generation sequencing (NGS) data analysis to accurately identify true mutations amid the inherent noise and errors in the sequencing process. </p> <p>The depth of sequencing refers to the number of times a given base in the genome is sequenced or covered by reads. By increasing the depth of sequencing, we improve the confidence in the called variants and enhance the ability to distinguish true mutations from sequencing artifacts or random errors. In regions with low coverage, the probability of missing genuine mutations increases, leading to false-negative results.  Therefore, a high depth of sequencing is crucial in NGS data analysis to ensure the reliability and accuracy of identified mutations and to gain deeper insights into the underlying genomic variations. As ONT data is relatively noisy compared to Illumina data, it is important to get a good coverage across your samples.</p> <p>Let's zoom into a region containing the rpoB gene. This gene is the target of rifampicin and mutation confering resistance often arise in thes gene. Type in \"rpoB\" into the search bar and hit enter to zoom into the correct region. You should see a view similar to the one below.</p> <p></p> <p>Colours are displayed on the coverage track if a non-reference nucleotide is present at &gt;= 20% of the total reads aligning to that position. These are potential candidates for containing a mutation. </p> <p>Question</p> <p>What differences do you notice between the two samples? What does this mean?</p> <p>Zoom into a few of the potential variant positions in sample1 as shown in the figure below. Can you identify which ones are error and which are true variants? </p> <p></p> <p>A simple method would be to use a majority consensus rule, whereby you take look at the allele which constitutes &gt;=50% of the total reads and check if it is different from the reference. Fill in the table below to see if you have any mutations. You can zoom to a specific potition my inputting the locus into the search bar (e.g. \"Chromosome:760497\") and hitting enter.</p> Position Reference Majority allele Variant 760497 C C No 761155 C 761422 A 761423 T 761656 C <p>Question</p> <p>It looks like sample2 has many more variants. Are these true variants or errors?</p> <p>While it is very useful to look at the sequencing data as we have above, however it is very time consuming as I'm sure you have noticed. While it is possible to do for a specific gene in a sample, this method is not feasible if we scale up to the 4000 genes and multiple samples. For this reason we have automated methods that do exactly as we have done. We will cover these methods in variant detection.</p> <ol> <li> <p>Cole, S. T. et al. Deciphering the biology of Mycobacterium tuberculosis from the complete genome sequence. Nature 393, 537\u2013544 (1998).\u00a0\u21a9</p> </li> </ol>"},{"location":"bioinformatics/phylogenetics/","title":"Phylogenetics","text":"<p>The underlying theme of this practical is the use of genomics in understanding TB drug resistance and strain types. The practical is split into three sections. Each using different programs and with a different data type(s). For each section try to:</p> <ul> <li>Be familiar with the programs presented and the types of analyses the can perform</li> <li>Understand the format of the input/output data</li> <li>Understand the conclusions you can draw from the analysis</li> <li>Think about how you can apply these concepts to different organisms/scenarios</li> </ul>"},{"location":"bioinformatics/phylogenetics/#whole-genome-phylogenetic-analysis","title":"Whole genome phylogenetic analysis","text":"<p>In this section we will be generating phylogenetic trees from whole-genome polymorphisms. The dataset consists of 51 M. tuberculosis isolates, which underwent DNA sequencing using Illumina (Genome Analyser II, 76-bp paired-end) technology. Samples were isolated from 41 treatment-experienced TB patients attending a clinic in Kampala, Uganda, including longitudinal samples from five patients and cases of multi drug-resistant TB (MDR-TB). Raw reads were mapped to the H37Rv reference genome using BWA software and subsequently 6857 SNPs inferred employing SAMTOOLS and BCF/VCFTOOLS. By concatenating the SNP locations for the 51 samples, a FASTA formatted dataset (uganda_gen.fasta) has been prepared to read into most phylogenetics software. Here we will be using iqtree to reconstruct the phylogeny.</p> <p>Phylogenetics is the study of the evolutionary relationships and history among species or groups of organisms. By examining genetic, morphological, and other types of data, scientists construct \"trees\" that depict these relationships. These trees, called phylogenetic trees or cladograms, represent hypotheses about the patterns of descent among lineages. Branch points, or nodes, on the tree indicate a common ancestor, while the length of branches can sometimes represent time or genetic change. Through phylogenetics, you can trace the lineage of specific species back to their most recent common ancestor and gain insights into evolutionary processes, patterns, and events.</p> <p>IQ-TREE is a widely-used software tool for the construction of phylogenetic trees using maximum likelihood methods. Developed as an alternative to traditional tools like RAxML and PhyML, IQ-TREE is recognized for its efficiency and speed, especially when handling large datasets. One of its distinguishing features is the ability to automatically determine the best-fitting substitution model, which simplifies the process for users. Additionally, IQ-TREE provides a range of advanced features and supports various types of analyses, making it a versatile choice in evolutionary biology and related fields.</p> <p>Forst activate the right conda environment and go to the direcotry containing all the files for this session:</p> <pre><code>conda activate tb-profiler\ncd ~/data/example_data/phylogenetics/exercise2\n</code></pre> <p>And then reconstruct the tree use the following command:</p> <pre><code>iqtree -s uganda_gen.fasta\n</code></pre> <p>Iqtree builds the phylogenetic tree by estimating the genetic distance between all isolates. This is based on the number of SNP differences observed between isolates. Isolates which have more differences between them will be more distantly related and isolates which are more similar have a close recent common ancestor.</p> <p>Nativate to https://itol.embl.de/ and click on \"upload\" on the top navbar. Click \"browse\" and select the file ending in \".treefile\" file that you produced using iqtree, followed by \u201cupload\u201d.</p> <p></p> <p>There are a few features in itol that we need to be aware of (labelled on the figure above):</p> <ol> <li>The contol panel: This is where you can change the way the tree is drawn and make stylistic changes.</li> <li>The sequence IDs: These IDs come from the fasta file used to build the tree</li> <li>The search button: We can search for the position of particular samples of interest using the search function.</li> </ol> <p>On the control panel, click on \"Advanced\", scroll all the way down and click on \"Midpoint root\". This will draw tree with the most recent common ancestor to all sequences being placed in the middle of the longest path between any two sequences in the tree.</p> <p>The identifiers on the right hand side of the tree (e.g. A70659) consist of the patient ID, where multiple samples have been taken from the same patient the ID is suffixed with a number indicating which timepoint (e.g. A70136_1). Isolates sequenced at different time points from the same patient are closely related and cluster together with very short branch lengths among them. This is the case of A70136, A70763 and A70144 patients pointed with green arrows in Figure 3. On the contrary, isolates from patient A70067 and A70011 (red arrows) happen to be placed at distant positions in the tree. In which terms could this be explained? We will now explore this for patient A70067.</p> <p>You have seen that genome alignments can be used to construct phylogenetic trees to understand the population structure of samples. They tend to cluster by strain-type in the Kampalan set, but this also holds across a global set of samples. M. tuberculosis can be classified into nine lineages, including four that are predominant; 1 Indo-Oceanic, 2 East-Asian including Beijing, 3 East-African-Indian, 4 Euro-American. These lineages are postulated to have differential roles in pathogenesis, disease outcome and variation in vaccine efficacy. For example, modern lineages, such as Beijing and Euro-American Haarlem strains exhibit more virulent phenotypes compared to ancient lineages, such as East African Indian.</p> <p>Here we use the Broad Institute's IGV genomic visualisation tool to consider SNP and structural variation differences between Kampalan Mtb alignments.</p> <p>Launch the IGV tool (typing <code>igv</code> on the command-line)</p> <p>We will be exploring the genomic data for samples A70067_1 (CAS2, lineage3, September 2003) and A70067_2 (T2, lineage 4, April 2004) which, as discussed in the previous exercise, were isolated from the same patient but are different strains being introduced at different times. We should now load in the BAM files (alignment files) from these two different strains in the previous loaded IGV environment.</p> <p>Click on File &gt; Load from file... and select the corresponding BAM files (A70067_1.bam and A70067_2.bam). Go to the chromosome region harbouring the Rv3738c gene (coordinates 4,189,463 to 4,190,410 bp) using the Search Box. Type in \u2018Rv3738c\u2019 or \u2018Chromosome:4,189,463-4,190,410\u2019 in the search box. </p> <p>The IGV screen should now be focused on that gene, but try zooming out and in.</p> <p></p> <p>Take a closer look at both the BAM and coverage tracks around Rv3738c gene. A deletion with respect to the reference can be spotted in A70067_1, whilst is absent in A70067_2. Several signatures are indicative of such event. The most obvious is the lack of read coverage at the region. The presence of read pairs with greater insert size (shown in brown) and badly aligned reads at both breakpoints are indicative of a large deletion too. </p> <p>Samples belonging to CAS2 strains (e.g. A70067_1) are reported to have a deletion covering this region while other strains types are not. These regions are often called Regions of Difference (RD), and some RDs and SNPs are associated with strain-types (Coll et al, 2014). </p> <p>As shown earlier, Isolates A70067_1 and A70067_2 exhibit different drug susceptibility profiles (see Table 1). Both isolates were tested for Isoniazid (INH) and Rifampicin (RIF) anti-TB drugs, with A70067_1 being susceptible and A70067_2 bi-resistant. </p> Patient Date SIT Spoligotype family (lineage) Drug     INH RIF Compared      to SNPs      All DR A70067_1 Sep-03 288 CAS2 (lineage 3) S  S H37Rv 1060 (539) 15 (9) A70067_2 Apr-04 2867 T2 (lineage 4) R  R H37Rv 475 (246) 11 (9) <p>Drugs: INH = Isoniazid , RIF=  Rifampicin, R = resistant, S = susceptible; DR = drug resistant candidates. SNP (non-synonymous changes). </p> <p>The primary mechanism for acquiring resistance in M. tuberculosis is the accumulation of point mutations (SNPs) in genes coding for drug targets or converting enzymes, and drug resistant disease arises through selection of mutants during inadequate treatment (Zhang &amp; Vilcheze, 2005) or new transmitted resistant strains. We will investigate polymorphism differences between A70067_1 and A70067_2 isolates in katG (Rv1908c, coordinates: 2153896-2156118 bp) and rpoB (Rv0667, coordinates: 759810-763328 bp) genes, known to be associated with INH and RIF resistance respectively.</p> <p>Make use of the Search Box functionality again to go to the genes of interest. Try to spot differences between the susceptible and resistant isolates in terms of presence/absence of SNPs. Note that lineage-specific SNPs may also be present. Table 2 contains drug resistance and lineage-specific SNPs found in the rpoB and katG genes. Mismatches are colour-coded, while nucleotides matching the reference are not. Nevertheless, not all mismatches are to be considered SNPs since some differences are due to sequencing errors. On average, 1 in every 1000 bases in the reads is expected to be incorrect. However, the high depth of coverage achieved by current sequencing platforms means SNPs can be distinguished from sequencing errors. True SNPs are expected to be mismatches occurring consistently across multiple reads at the same reference position, whereas mismatches at spurious locations are likely to be caused by sequencing errors.</p> <p></p> Gene Locus Name Chromosome position Nucleotide change Amino acid change and codon number Annotation katG Rv1908c 2155168 C/A S315I INH resistance conferring SNP katG Rv1908c 2155168 C/T S315N INH resistance conferring SNP katG Rv1908c 2155168 C/G S315T INH resistance conferring SNP katG Rv1908c 2154724 C/A R463L Non-lineage 4 specific SNP rpoB Rv0667 761155 C/T S450L RMP resistance conferring SNP rpoB Rv0667 761155 C/G D450W RMP resistance conferring SNP rpoB Rv0667 762434 T/G G876G CAS/lineage 3 specific SNP rpoB Rv0667 763031 T/C A1075A Non-lineage 4 specific SNP"},{"location":"bioinformatics/phylogenetics/#investigating-transmission","title":"Investigating transmission","text":"<p>Using web-based tools is a great way to run software without the need for installation or knowledge of Linux or the command-line. However, these are often not convenient to use if you have many samples, or don\u2019t have access to the internet. Many of the tools that we have used today are also available as command-line software (or have equivalents). To demonstrate this, we will run tb-profiler on our terminal. </p> <pre><code>tb-profiler profile --read1 A70067_1.fastq.gz --prefix A70067_1 --txt\n</code></pre> <p>There are a few arguments that we have given:</p> <ul> <li><code>--read1</code> : This allows us to specify the input fastq file</li> <li><code>--prefix</code> : This specifies the prefix for the output files</li> <li><code>--txt</code> : This ensures a text output will be created </li> </ul> <p>Try running the command for A70067_2 by changing the appropriate parameters.</p> <p>Question</p> <p>Have a look at both result files (in the results folder).  Do the profiles look different?</p> <p>Now we\u2019ll run tb-profiler on all samples. Running it from fastq files can take some time as it must go through all processing steps including trimming, mapping and variant calling. As a result it can take a while to run. Tb-profiler also can take input from vcf files which just contain variants. To run the pipeline using the provided vcfs the command should look like:</p> <pre><code>tb-profiler profile --vcf A70067_1.vcf.gz --prefix A70067_1 --txt\n</code></pre> <p>Here we have just switched the <code>--read1</code> argument for the <code>--vcf</code> argument. </p> <p>To establish if transmission has occurred, we frequently calculate snp-distance between pairs of samples and pairs with a distance less than a pre-defined cutoff (e.g. 10 snps) can be linked. We can investigate this by telling tb-profiler to calculate snp-distances and save the results of any pairs below a threshold. To do we can add <code>--snp_dist 20</code> to the command. We have used a relaxed value of 20 here as we can always make this more stringent after.</p> <p>Finally, it can be a bit repetitive to run the same command for many isolates. Ideally, we would like to harness the power of automation that is possible on the command-line so that we can profile all samples with just one command. Tb-profiler contains a batch mode that enables you set up your commands using a csv file that tells the pipeline where to look for the input files. To do this we\u2019ll create a csv file using some command-line tools:</p> <pre><code>ls *.vcf.gz | awk 'BEGIN {print \"id,vcf\"} {print $1\",\"$1}' | sed 's/.vcf.gz//' &gt; vcf_files.csv\n</code></pre> <p>This will create a csv file with two columns: id and vcf. Have a look at the format by running <code>head vcf_files.csv</code>. Now you can tell tb-profiler to run the pipeline for all samples in the csv file by running:</p> <pre><code>tb-profiler batch --csv vcf_files.csv --args \"--snp_dist 20\"\n</code></pre> <p>You\u2019ll notice we didn\u2019t give the <code>--vcf</code> and <code>--prefix</code> arguments as these are read from the csv file. We do have to specify any additional arguments with <code>--args</code> followed by the arguments as you would type them for a single sample command.</p> <p>After it has finished running we can again combine the output files into a single report by running:</p> <pre><code>tb-profiler collate\n</code></pre> <p>This will produce in addition to the tbprofiler.txt output file it will also create a file called tbprofiler.transmission_graph.json. We will visualise this we a web-based tool that you can open in your web browser at https://jodyphelan.github.io/tgv</p> <p>After the page loads click on the upload box and select the .json file. This represents the samples as nodes and where pairs of samples have a snp-distance less or equal to the cutoff an edge will be drawn between them (Figure 10). </p> <p>Question</p> <p>Does this agree with the phylogenetic tree? What happens if you alter the snp-distance cutoff to be more conservative (e.g. 10 SNPs)? What do you think is the best cutoff to use?</p> <p></p> <p>Finally, the collate command also produces annotation config files that can be used with iTOL. To load the annotations simple drag and drop the following files onto the tree that you loaded earlier: * tbprofiler.lineage.itol.txt \u2013 a strip with the main lineage annotated. * tbprofiler.dr.itol.txt \u2013 a strip indicating the drug resistance type. * tbprofiler.dr.individ.itol.txt \u2013 circles for each individual drug where a solid filled circle indicates resistance.</p> <p></p>"},{"location":"bioinformatics/tb-profiler/","title":"TB-profiler","text":"<p>In the previous sessions you have performed mapping, variant calling and annotation to find drug resistance variants in Mtb data. We also looked at the quality of the data through coverage- and depth-based metrics. A lot of commands and manual work were used to get to this final result. To make this process easier we have developed a tool called tb-profiler to automate this process and provide an end-to-end pipeline for TB resistance prediction from sequence data. The pipeline searches for small variants and big deletions associated with drug resistance and will also report the strain type (lineage). </p>"},{"location":"bioinformatics/tb-profiler/#profiling","title":"Profiling","text":"<p>Let's activate the tb-profiler environment and create a folder for our results:</p> <pre><code>cd ~/data/example_data\nmkdir tb-profiler\ncd tb-profiler\n</code></pre> <p>Now we can run tb-profiler using the following command for sample1:</p> <pre><code>tb-profiler profile -1 ../sample1.fastq.gz --platform nanopore --prefix sample1 --txt\n</code></pre> <p>Let's break down this command:</p> <ol> <li><code>profile</code>: this tells the tool to use the profile function, which runs the whole pipeline </li> <li><code>-1 ../sample1.fastq.gz</code>: used to provide the fastq files</li> <li><code>--platform nanopore</code>: tells the pipeline that we are using ONT data, the default for this parameter is Illumina</li> <li><code>--prefix sample1</code>: this argument allows you to provide a prefix to the resulting output files</li> <li><code>--txt</code>: specified that we want the reuslt file in text format</li> </ol> <p>The results will be put into the results folder. Have a look at the text output using less:</p> <pre><code>less results/sample1.results.txt\n</code></pre> <p>There are a few sections in the output report:</p> <ul> <li>Summary: provides a brief summary of the the drug resistance and strain type.</li> <li>Resistance report: For each drug lists any variants found and the proportion at which they occured in the raw data. Multiple variants across different genes can arise for the same drug.</li> <li>Resistance variants report: similar to the section above but each line represents one variant and more information such as the genome position and variant type is provided.</li> <li>Other variants report: lists all variants in drug resistance candidate genes that are not associated with resistance.</li> </ul> <p>Exercise</p> <p>Run the profiling for the other samples. Can you see any differences?</p>"},{"location":"bioinformatics/tb-profiler/#combining-reports","title":"Combining reports","text":"<p>The results from numerous runs can be collated into one table using the following command:</p> <pre><code>tb-profiler collate\n</code></pre> <p>This will automatically create a number of colled result files from all the individual result files in the result directory. You should now see several files that have been created in your current directory. Open up the main summary file using using this link</p> <p>This will open an open source alternative to excel called libreoffice. When the file is opened you should one line for each sample. The columns give you information about:</p> <ul> <li>Lineage and sublineage information (strain types)</li> <li>QC metrics such as median depth and number of reads mapping</li> <li>Number of variants found</li> <li>Columns representing the drugs and mutations found</li> </ul>"},{"location":"bioinformatics/tb-profiler/#customising-outputs","title":"Customising outputs","text":"<p>By default a .json formatted output file is produced in a directory called results. This file contains information including mutations found, lineage as well as some QC metrics. This format is perfect to load into script for downstream processing but it isn't very human-readable. For a more human-readlable format you can use the --csv and --txt flags to generate outputs in csv and text format too. These files will contain several tables listing similar information to the json format. For advanced users, it is possible to customise this format by providing a template file. It is also possible produce a nice-looking docx formatted report that can be viewed in Word or converted into pdf. The advantage of this is that it can contain images, text formatting, etc. To create this report, a template file must be provided. As with the text custom format, the docx template should have jinja variables defined that will be filled in with sample data when a report is generated. </p> <p>First lets download a template:</p> <pre><code>wget https://github.com/jodyphelan/tb-profiler-templates/raw/main/docx/brti_template.docx\n</code></pre> <p>Now we can use tb-profiler to use fill in this template with the information from sample1:</p> <pre><code>tb-profiler reformat results/sample1.results.json --docx --docx_template brti_template.docx\n</code></pre> <p>You should now have a word file called sample1.results.docx in your directory. Open this file by clicking on this link</p> <p>Question</p> <p>Do you see the same information as with the text report?</p> <p>Question</p> <p>Try to modify the original template file (brti_template.docx) and rerun the <code>tb-profiler reformat</code> step. Did it work?</p> <p>Exercise</p> <p>Are there any discrepancies between the drug resistance predictions based on WGS from tb-profiler and the phenotypic DST results?</p>"},{"location":"bioinformatics/variant-calling/","title":"Variant calling Oxford Nanopore sequencing data","text":""},{"location":"bioinformatics/variant-calling/#intended-learning-outcomes","title":"Intended learning outcomes","text":"<p>This practical session goes into how to perform variant calling on long-read sequencing data from ONT:</p> <ol> <li>Learn the concepts behind variant calling</li> <li>ONT data specific tools for variant calling</li> <li>Visualising variants in genomic contexts </li> </ol>"},{"location":"bioinformatics/variant-calling/#background","title":"Background","text":"<p>Variants in genomic data refer to differences in the sequence of DNA or RNA between individuals or between a sample and a reference genome. These differences can manifest as single nucleotide polymorphisms (SNPs), insertions, deletions, or more complex structural variations. Given that you're familiar with sequencing technologies and mapping, let's delve straight into the concept of variant calling, emphasizing its application in Oxford Nanopore sequencing data.</p> <p></p>"},{"location":"bioinformatics/variant-calling/#variant-calling-a-birds-eye-view","title":"Variant Calling: A Bird\u2019s-Eye View","text":"<p>Once you've sequenced your DNA sample and mapped the reads onto a reference genome, the next step is to identify where your sample differs from this reference. This process is known as variant calling. Conceptually, it's like comparing two texts and highlighting where they differ. In this genomic context, these differences \u2014 or variants \u2014 can be tiny, like a single changed letter (a SNP), or larger, like a whole sentence being added or removed (an insertion or deletion).</p>"},{"location":"bioinformatics/variant-calling/#challenges-with-oxford-nanopore-sequencing","title":"Challenges with Oxford Nanopore Sequencing","text":"<p>Oxford Nanopore Technologies (ONT) provides a unique sequencing approach, producing long reads that can span thousands to tens of thousands of bases. This is a game-changer for detecting structural variants and sequencing challenging regions of the genome. However, there's a catch: these long reads come with a higher error rate compared to other sequencing technologies.</p> <p>The errors introduced during ONT sequencing can be substitutions, insertions, or deletions. This can make variant calling more challenging because we need to differentiate between genuine variants and errors introduced by the sequencing process.</p>"},{"location":"bioinformatics/variant-calling/#preventing-false-variants-strategies-to-remember","title":"Preventing False Variants: Strategies to Remember","text":"<p>Given the error-prone nature of Oxford Nanopore sequencing, strategies have been developed to distinguish real variants from sequencing errors:</p> <ol> <li> <p>Depth of Coverage: One of the simplest ways to differentiate between true variants and errors is by looking at the depth of coverage. A genuine variant will typically be observed in multiple reads covering the same position, while a sequencing error might only appear in one or a few reads. Sometime we set a hard cut-off, where for example, 70% of the bases at a position would have to support a variant call.</p> </li> <li> <p>Statistical Modeling: Many modern variant callers employ statistical models to evaluate the likelihood that a detected variant is real. These models consider various factors, such as the quality score of the bases in the reads, the depth of coverage, and the expected error rate of the sequencing technology.</p> </li> <li> <p>Using a Combination of Sequencing Technologies: If resources allow, combining the long-read data from ONT with short-read data from platforms like Illumina can provide a more comprehensive and accurate variant landscape. The short-read data, with its lower error rate, can validate variants detected by the long reads. This is called hybrid sequencing analysis, which is very powerful when both long-reads and high-fidelity is required.</p> </li> <li> <p>Repeat &amp; Validate: Especially for critical findings, consider re-sequencing the sample or using a different method (like PCR followed by Sanger sequencing) to validate the identified variants.</p> </li> </ol>"},{"location":"bioinformatics/variant-calling/#in-conclusion","title":"In Conclusion","text":"<p>Variant calling in Oxford Nanopore sequencing data poses unique challenges due to its error-prone nature. But with careful data processing, the use of specialized tools designed for long reads, and strategies to discern real variants from errors, researchers can extract valuable genetic insights from their samples. As with any genomic analysis, it's always a good practice to stay updated with the latest tools and methodologies, and always cross-check critical results for the highest accuracy.</p>"},{"location":"bioinformatics/variant-calling/#activity","title":"Activity","text":"<p>We are going to take the three TB samples you mapped in the previous activity and perform variant calling. There are many tools available for variant calling. The most common are Freebayes or GATK, but these are used primarily for short-read data. We are going to use nanopore-specific tools. Options include ONT's own Medaka or Clair3 which both incorporate aspects of the complex basecalling models to avoid making low-frequency or false calls. However, for ease, we will use another tools adapted for variant calling nanopore data called Pilon.</p>"},{"location":"bioinformatics/variant-calling/#vcfs","title":"VCFs","text":"<p>All variant callers create outputs called a Variant Call File (VCF). This file contains variant positions in a BAM alignment, where the reads mapped do not match the reference sequence. The variant caller will scan each position in an alignment and quantify the bases supporting the REF or bases supporting and alternative (ALT) variant call. Below is an example of a VCF, which we will generate in the next step. Take a look at the features and try to understand their importance. </p> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 1 12345 . A G 29 PASS DP=20 GT 0/1 1 67890 . T C 3 PASS DP=15 GT 1/1 2 54321 . G A 60 PASS DP=25 GT 0/0 <ol> <li> <p><code>#CHROM</code> (Chromosome)</p> <ul> <li>Name of the chromosome (e.g., 1-22, X, Y, or MT for humans).</li> </ul> </li> <li> <p><code>POS</code> (Position)</p> <ul> <li>The reference position, starting from 1.</li> </ul> </li> <li> <p><code>ID</code></p> <ul> <li>Variant identifiers. Defaults to a dot (<code>.</code>) if none.</li> </ul> </li> <li> <p><code>REF</code> (Reference Base(s))</p> <ul> <li>Reference base(s) for the variant. For insertions, it's the base before the insertion.</li> </ul> </li> <li> <p><code>ALT</code> (Alternative Base(s))</p> <ul> <li>Comma-separated list of the alternative allele(s). Defaults to a dot (<code>.</code>) if none.</li> </ul> </li> <li> <p><code>QUAL</code> (Quality)</p> <ul> <li>Phred-scaled quality score. Defaults to a dot (<code>.</code>) if unknown.</li> </ul> </li> <li> <p><code>FILTER</code></p> <ul> <li>Filters that the variant has passed or failed. \"PASS\" if all filters are passed. Defaults to a dot (<code>.</code>) if filtering hasn't been applied.</li> </ul> </li> <li> <p><code>INFO</code></p> <ul> <li>Additional information about the variant. Described in meta-information lines (e.g., <code>##INFO</code>).</li> </ul> </li> <li> <p><code>FORMAT</code></p> <ul> <li>Describes the data types and order for the per-sample columns.</li> </ul> </li> <li> <p><code>SAMPLE(s)</code></p> <ul> <li>Data for each sample. The data types and order are defined in the <code>FORMAT</code> field.</li> </ul> </li> </ol>"},{"location":"bioinformatics/variant-calling/#calling-variants-for-our-tb-samples","title":"Calling variants for our TB samples","text":"<p>First, you want to activate the relevant conda environment. Activate the tb-profiler environment to load all of the programs required for variant calling:</p> <pre><code>conda activate tb-profiler\n</code></pre> <p>We want to make sure we are in the folder with all of the alignments you generated in the previous activity. Move to the 'example_data' folder:</p> <pre><code>cd ~/data/example_data/\n</code></pre> <p>With Pilon, we can set a hard cut-off limit for the variants we accept. Given that we have used nanopore sequencing, we set this high to avoid picking up erroneous variants.</p> <p>Run Pilon now on the three BAMs created in the previous session</p> <pre><code>pilon -Xmx2g --genome ~/refgenome/tb.fasta --nanopore sample1.bam --variant --output sample1 --mindepth 20\n</code></pre> <p>We use the <code>-Xmx2g</code> argument to provide 2 gb of RAM to pilon. If your VM/computer doesn't have enough RAM this step might fail. </p> <p>This command will output every site in the genome including those at which there are no variants and those at which there is a low amount of evidence. To get high quality variants we can extract only those which occur in &gt;70% of the reads.</p> <pre><code>bcftools view -i 'AF&gt;0.7' sample1.vcf &gt; sample1.filt.vcf\n</code></pre> <p>Now that the variant calling is complete, you can open the VCF files and inspect the variants. If you were writing a paper on the prevalence of mutations in a population, you could use this output to make a table reporting allele frequencies of mutations.</p> <pre><code>less sample1.filt.vcf\n</code></pre> <p>If you want to view this information as a neater table we can use the bcftools query function which allows you to query the vcf and extract specific information about each variant</p> <pre><code>bcftools query -f '%POS %REF %ALT %AF\\n' sample1.filt.vcf\n</code></pre> <p>This will extract the position, reference, alternate and the frequency of the alternate allele in the raw data. Bcftools is very useful when you want to extract specific information in a table format that you can then feed into downstream analyses. </p> <p>We can visualise the variant calls alongside the mapping data you have created. With IGV open, and your alignment loaded, add the VCF track, and navigate to position 761155. Here we can see where the variant caller has found a position with &gt; 70% of the bases supporting the ALT allele. Navigate to position 760497. Here we can see where a variant has not made the cut. In this case, the ALT frequency was &lt; 70%, therefore, it was not included as a variant in our VCF.</p> <p></p> <p></p>"},{"location":"bioinformatics/variant-calling/#consequence-calling","title":"Consequence calling","text":"<p>Drug resistance in Mtb is cause by mutations in drug targets and drug converting enzymes. Below is a non-exhaustive list of drugs and assosciated resistance genes. </p> Drug Gene Isoniazid katG, fabG1 promoter, inhA Rifampicin rpoB, rpoC Ethambutol embB, embC, embA, Ethionamide fabG1 promoter, inhA, ethA Pyrazinamide pncA, pncA-Rv2044c Streptomycin rpsL, gid, rrs Aminoglycosides rrs, eis promoter, tlyA Fluroquinolones gyrA, gyrB Cycloserine alr, ald PAS thyA, thyX promoter, folC, ribB <p>We will try to look through our variant list to find out if any mutations in these candidate genes exist. The first step in this process is to annotate the gene each mutations is located on. To do this we can use a tool called snpEff which annotates genes and also converts the nucleotide change into protein coding changes. The input is a VCF file and the output is the same VCF with the additional information added using the \"ANN\" tag in the INFO field. We can run this tool with the following command:</p> <pre><code>snpEff ann Mycobacterium_tuberculosis_h37rv -no-downstream -ud 50 sample1.filt.vcf &gt; sample1.filt.ann.vcf\n</code></pre> <p>The <code>-no-downstream</code> arguments prevents variants from being annotated with downstream gene consequences as these are rarely significant in tb. The <code>-ud 100</code> if a variant is upstream of a gene it will only be annotated with that gene if it falls within 100 nucleotides of the start. This  ensures that genes occuring in the promoter of a gene will be annotated correctly but anything further away thatn 100 nt from a gene start is unlikely to have an effect on that gene.</p> <p>We can again extract the relevant information from the VCF file with  <code>bcftools query</code>:</p> <pre><code>bcftools query -f '%POS %REF %ALT %AF %ANN\\n' sample1.filt.ann.vcf &gt; sample1.mutations.txt\n</code></pre> <p>Exercise</p> <p>Check this file to see if we have any resistance variants presented in the table below. You can do this by using a tool called <code>grep</code> to search for specific text in a file. For example:</p> <pre><code>grep inhA sample1.mutations.txt\n</code></pre> <p>What pattern of resistance does this sample have?</p> Gene Mutation Drug inhA p.Ile194Thr isoniazid inhA p.Ile21Asn isoniazid inhA p.Ile21Thr isoniazid inhA p.Ile21Val isoniazid inhA p.Ser94Leu isoniazid inhA p.Ser94Ala isoniazid inhA p.Ser94Trp isoniazid rpoB p.Ser450Tyr rifampicin rpoB p.Ser450Phe rifampicin rpoB p.Ser450Leu rifampicin rpoB p.Ser450Met rifampicin rpoB p.Ser431Gly rifampicin rpoB p.Ser431Thr rifampicin rpoB p.Ser431Arg rifampicin"},{"location":"extra-reading/about_conda/","title":"Conda Environments: Why and How to Use Them","text":""},{"location":"extra-reading/about_conda/#what-is-a-conda-environment","title":"What is a Conda Environment?","text":"<p>Conda is an open-source package manager and environment manager that allows users to install multiple versions of software packages and their dependencies in isolated environments. A conda environment is essentially a directory that contains a specific collection of conda packages.</p>"},{"location":"extra-reading/about_conda/#why-use-conda-environments","title":"Why Use Conda Environments?","text":"<ol> <li>Isolation: Multiple projects may require different versions of libraries or even the Python interpreter itself. By using separate environments, you can ensure that each project has its own dependencies, isolated from each other.</li> <li>Avoid Conflicts: Dependencies of one project might conflict with another. Environments ensure that such conflicts are avoided by isolating dependencies.</li> <li>Reproducibility: Sharing a project with someone else (or even with your future self) becomes much simpler when you can provide a list of exact package versions used in your environment.</li> <li>Flexibility: You can easily switch between different versions of a package or Python itself, depending on the project's needs.</li> <li>Clean and Safe Experimentation: If you want to test out a new package or update an existing one, you can do so in a new environment. If something goes wrong, your main working environment remains unaffected.</li> </ol>"},{"location":"extra-reading/about_conda/#how-does-it-work","title":"How Does It Work?","text":"<p>Conda environments work by creating isolated spaces, each with its own installation directories. This allows you to have different versions of a package (or even Python itself) in different environments. When you activate an environment, conda adjusts your <code>PATH</code> so that the selected environment's executables are used.</p>"},{"location":"extra-reading/about_conda/#some-basic-conda-commands","title":"Some Basic Conda Commands:","text":"<ol> <li>Creating a New Environment: <code>conda create --name myenv python=3.7</code></li> <li>Activating an Environment:  </li> <li>On macOS and Linux: <code>source activate myenv</code> or <code>conda activate myenv</code></li> <li>Deactivating an Environment: <code>conda deactivate</code></li> <li>Listing All Environments: <code>conda env list</code> or <code>conda info --envs</code></li> <li>Installing a Package in an Active Environment: <code>conda install numpy</code></li> <li>Exporting an Environment to a YAML File: <code>conda env export &gt; environment.yml</code></li> <li>Creating an Environment from a YAML File: <code>conda env create -f environment.yml</code></li> <li>Removing an Environment: <code>conda env remove --name myenv</code></li> </ol>"},{"location":"extra-reading/about_conda/#conclusion","title":"Conclusion","text":"<p>Conda environments are essential tools for data scientists and developers to manage dependencies and ensure project reproducibility. By understanding and utilizing them, you can maintain a more organized, conflict-free, and efficient development workflow.</p>"},{"location":"lab-protocols/","title":"Lab protocols","text":""},{"location":"lab-protocols/#asdasd","title":"asdasd","text":""},{"location":"other-stuff/item1/","title":"item1","text":""},{"location":"other-stuff/item2/","title":"item 2","text":""}]}